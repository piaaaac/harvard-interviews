Title: Cognitive and Interpersonal Factors Related to Success on Collaborative Creative Projects

----

Text:

This longitudinal project tracked common decision biases and factors that supported or inhibited progress on 300+ professional team-based innovative projects. The full dataset is comprised of interviews and observations of over 150 creative professionals. It includes projects from 176 companies, 23 industries, 8 countries, and 5 continents.

Projects were categorized as successful or unsuccessful, based on the innovativeness of the idea. Successfully innovative projects were defined similarly to a patent definition, in that they: a) solve a problem, and b) do so in an original way. Success was judged from the perspective of the people who worked on the project, as well as multiple expert raters.

Tags were created using academic research on team-based innovation and creativity. These include individual traits and team-level factors that have been previously linked to innovation success and failure, as well as cognitive decision-making biases that the principal investigator hypothesized would affect the process and outcome of innovation projects. There are over 180 known cognitive (decision-making) biases and heuristics. We narrowed that list down to 86 biases to track as highly relevant to team based creative projects. Only 65 of these showed up with at least one tag. In addition to these granular tags, we identified 25 metadata categories. Metadata tags make it possible to compare teams with similar characteristics and see what patterns of biases and common pitfalls occur most frequently. Example metadata tags include team size, industry, time pressure, budget pressure, team dynamics, number of ideas generated, etc. Metadata categories can be used to compare tags associated with projects in the same industry or what types of tags show up when budget was an issue on a team. 

Data collected from interviews and observations were transcribed, translated, and cleaned in preparation for ethnographic coding (similar to tagging). Tagging qualitative data for meaning is a time-consuming process. All of the tagging was done by human raters using Dovetail App software that is designed to help discover patterns in qualitative data. When tagging qualitative interview data, each transcript was read in its entirety twice by each reviewer. In the first round, the interviewer identified anything they found obviously matched a tag and marked other text to revisit later that seemed important but not yet clear. In the second tagging round, each reviewer resolved any question mark tags and checked for any errors in the original tags. After each rater completed their tagging, we used inter-rater reliability to test the degree to which independent raters agreed on the meaning of the text. The raters showed very high (80%) tagging agreement, which gives us confidence that human tagging was objective enough for our results to be valid.